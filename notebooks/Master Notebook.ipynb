{
 "metadata": {
  "name": "",
  "signature": "sha256:9773bf76fde92dc93cac748d432af45e1b2ce19bcdadc27d2c1846c93a9f3514"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import pickle\n",
      "import nltk\n",
      "import string\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from nltk.corpus import wordnet as wn\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn import datasets, linear_model\n",
      "sys.path.append('../Scripts')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# loading data from pickle files (tokenization & tagging- default nltk)\n",
      "import ted_object\n",
      "from ted_object import ted\n",
      "t_obj = ted()\n",
      "\n",
      "# Corpus objects\n",
      "tagged_sents = t_obj.tagged_sents_talk()\n",
      "tagged_words = t_obj.tagged_words_talk()\n",
      "tagged_paras = t_obj.tagged_paras()\n",
      "#words_bag = t_obj.words_bag()\n",
      "\n",
      "# y labels\n",
      "all_labels = pickle.load(open('../Corpus/y.p', 'rb'))\n",
      "confusing_y = [row[0] for row in all_labels]\n",
      "funny_y = [row[1] for row in all_labels]\n",
      "informative_y = [row[2] for row in all_labels]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Normalize text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import *\n",
      "from nltk.stem.porter import *\n",
      "\n",
      "def tokenize_text(text):\n",
      "    \"\"\"Retains '(laughs)'\"\"\"\n",
      "    pattern =[\"(?x)([A-Z]\\.)+\",\n",
      "               \"\\w+([-]\\w+)*\",\n",
      "               \"\\$?\\d+(\\.\\d+)?%?\",\n",
      "               \"\\.\\.\\.\",\n",
      "               \"[.,?;]+\"]\n",
      "    pattern = \"|\".join(pattern)\n",
      "    text = \" \".join(text) if isinstance(text, list) == True else text\n",
      "    tokens = nltk.regexp_tokenize(text,pattern)\n",
      "    return tokens\n",
      "\n",
      "wnl = WordNetLemmatizer()\n",
      "\n",
      "# reduce words to root format\n",
      "def get_root_phrase(word, tag):\n",
      "    word = word.lower()\n",
      "    #trim joint words\n",
      "    if word.endswith(\"'s\"):\n",
      "        print word\n",
      "        word = word[:-2]\n",
      "    #don't cut short words. e.g. rss would be reduced to r\n",
      "    if len(word)<=3 and word not in COMMON_WORDS:\n",
      "        return word\n",
      "    #try to morphy\n",
      "    morphy = wn.morphy(word)\n",
      "    if morphy is None:\n",
      "        morphy = word\n",
      "    #lemmatize morphy\n",
      "    lem = wnl.lemmatize(morphy, 'n')\n",
      "    if lem and len(word)>3: \n",
      "        morphy = lem\n",
      "    return morphy\n",
      "\n",
      "# Get morphy words in a sentence\n",
      "def get_morphys(tagged_sentence):\n",
      "   return [ (get_root_phrase(word, tag), tag) for (word, tag) in tagged_sentence \\\n",
      "               if word not in stop_words and (word.isalnum() or '-' in word) and not word.isdigit()\\\n",
      "               and len(get_root_phrase(word, tag))>1]\n",
      "\n",
      "#remove all stop words and punctuations\n",
      "non_stop_words = list()\n",
      "for talk in tagged_words:\n",
      "    non_stop_words.append([(w,t) for w,t in talk if w.lower() not in stopwords.words('english') and\n",
      "                       w.lower() not in string.punctuation])\n",
      "pickle.dump(non_stop_words, open( \"../Corpus/non_stop_tagged_words_talks.p\", \"wb\" ))\n",
      "\n",
      "porter_stemmer = PorterStemmer()\n",
      "stemmed_words = list()\n",
      "for talk in non_stop_words:\n",
      "    stemmed_words.append([(porter_stemmer.stem(w),t) for w,t in talk])\n",
      "pickle.dump(stemmed_words, open( \"../Corpus/stemmed_tagged_words_talks.p\", \"wb\" ))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Helper functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#helper function to chunk the sentenses based on a grammar\n",
      "def parse_sents(ip_sent, grammar):\n",
      "    cp = nltk.RegexpParser(grammar)\n",
      "    result = cp.parse(ip_sent)\n",
      "    return result\n",
      "\n",
      "#helper function to extract subtrees\n",
      "def extract_tree(sents,grammar, match):\n",
      "    ret_list = []\n",
      "    for sent in sents:\n",
      "        res = parse_sents(sent, grammar)\n",
      "        ret_list += [' '.join([word for(word, tag) in subtree]) for subtree in res.subtrees(filter = lambda t: t.node == match)]\n",
      "    return ret_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Feature Extraction Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NEGATION_LIST = [\"no\", \"not\", \"never\", \"can't\", \"won't\"]\n",
      "CERTAINITY_LIST = [\"always\", \"never\", \"often\", \"sometimes\"]\n",
      "HEDGE_LIST = [\"well\", \"possibly\", \"maybe\"]\n",
      "\n",
      "def min_max_normalize(feature):\n",
      "    '''Converts a list of numberical values to a list of values between 0 and 1'''\n",
      "    minVal = min(feature)\n",
      "    maxVal = max(feature)\n",
      "    normalized = [(e - minVal / (maxVal - minVal)) for e in feature]\n",
      "    return normalized\n",
      "\n",
      "\n",
      "def punctuation_count(talk_sents, punc):\n",
      "    featureVector = []\n",
      "    for talk in talk_sents:\n",
      "        num_punc = len([1 for sent in talk for w,t in sent if punc == w])\n",
      "        featureVector.append(num_punc)\n",
      "    return featureVector\n",
      "\n",
      "# From the paper Rahul mentioned for Technical terms - Citation required\n",
      "def num_techterms(talk_sents):\n",
      "    featureVector = []\n",
      "    for talk in talk_sents:\n",
      "        njn_list = extract_tree(talk, 'NJN: {(<N.*>*<J*>*<N.*>+)+}','NJN')\n",
      "        npn_list = extract_tree(talk, 'NPN: {(<N.*>+<P.*><N.*>+)+}','NPN')\n",
      "        featureVector.append(sum(len(njn_list), len(npn_list)))\n",
      "    return featureVector\n",
      "\n",
      "def num_first_person_pronoun_chunk(talk_sents):\n",
      "    featureVector = []\n",
      "    pro_verb = \"(<PR.*>|<N.*P>+)+<TO>?<MD>?<TO>?<VB.*>+\"\n",
      "    VB_grammar = \"PROVB: {\" + pro_verb + \"}\"\n",
      "    for talk in talk_sents:\n",
      "        pvb_list = extract_tree(talk,VB_grammar, \"PROVB\")\n",
      "        featureVector.append(len(pvb_list))\n",
      "    return featureVector    \n",
      "\n",
      "def num_words(talk_sents):\n",
      "    featureVector = []\n",
      "    for talk in talk_sents:\n",
      "        word_list = [word for sents in talk for word in sents]\n",
      "        num_words = len(word_list)        \n",
      "        featureVector.append(num_words)\n",
      "    return featureVector\n",
      "\n",
      "def avg_word_length(talk_sents):\n",
      "    featureVector = []\n",
      "    for talk in talk_sents:\n",
      "        word_list = [word for sents in talk for word in sents]\n",
      "        num_words = len(word_list)    \n",
      "        total_len = sum([len(sent) for sent in talk])\n",
      "        featureVector.append(total_len / num_words)\n",
      "    return featureVector\n",
      "\n",
      "def num_unique_words(talk_sents):\n",
      "    featureVector = []\n",
      "    for talk in talk_sents:\n",
      "        word_list = [word for sents in talk for word in sents]\n",
      "        num_unique = len(list(set(word_list)))\n",
      "        featureVector.append(num_unique)\n",
      "    return featureVector\n",
      "\n",
      "def num_unique_big_words(talk_sents):\n",
      "    featureVector = []\n",
      "    for talk in talk_sents:\n",
      "        word_list = [word for sents in talk for word in sents]\n",
      "        unique_big = len(list(set([word for word in word_list if len(word) > 10])))\n",
      "        featureVector.append(unique_big)\n",
      "    return featureVector\n",
      "\n",
      "def num_neg_words(talk_sents):\n",
      "    featureVector = []\n",
      "    for talk in talk_sents:\n",
      "        word_list = [word for sents in talk for word in sents]\n",
      "        num_neg = len([word for word in word_list if word in NEGATION_LIST])\n",
      "        featureVector.append(num_neg)\n",
      "    return featureVector\n",
      "\n",
      "def num_certain_words(talk_sents):\n",
      "    featureVector = []\n",
      "    for talk in talk_sents:\n",
      "        word_list = [word for sents in talk for word in sents]\n",
      "        num_certain = len([word for word in word_list if word in CERTAINTY_LIST])\n",
      "        featureVector.append(num_certain)\n",
      "    return featureVector\n",
      "\n",
      "def num_hedge_words(talk_sents):\n",
      "    featureVector = []\n",
      "    for talk in talk_sents:\n",
      "        word_list = [word for sents in talk for word in sents]\n",
      "        num_hedge = len([word for word in word_list if word in HEDGE_LIST])\n",
      "        featureVector.append(num_hedge)\n",
      "    return featureVector\n",
      "\n",
      "def num_sents(talk_sents):\n",
      "    featureVector = []\n",
      "    for talk in talk_sents:\n",
      "        featureVector.append(len(talk))\n",
      "    return featureVector\n",
      "\n",
      "def avg_words_sent(talk_sents):\n",
      "    featureVector = []\n",
      "    for talk in talk_sents:\n",
      "        word_list = [word for sents in talk for word in sents]\n",
      "        avg = len(word_list) / len(talk)\n",
      "        featureVector.append(avg)\n",
      "    return featureVector\n",
      "\n",
      "def num_paras(talk_paras):\n",
      "    featureVector = []\n",
      "    for talk in talk_paras:\n",
      "        featureVector.append(len(talk))\n",
      "    return featureVector\n",
      "\n",
      "def avg_words_para(talk_paras):\n",
      "    featureVector = []\n",
      "    for talk in talk_paras:\n",
      "        word_list = [word for paras in talk for sent in paras for word in sent]\n",
      "        avg = len(word_list) / len(talk)\n",
      "        featureVector.append(avg)\n",
      "    return featureVector\n",
      "\n",
      "def count_pos(tagged_words, pos_tag):\n",
      "    \"\"\"Counts number of occurrences of a particular POS in each talk\"\"\"\n",
      "    featureVector = []\n",
      "    for talk in tagged_words:\n",
      "        count = 0\n",
      "        for word , tag in talk:\n",
      "            if pos_tag in tag:\n",
      "                count += 1\n",
      "        featureVector.append(count)\n",
      "    return featureVector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def confusing_matrix():\n",
      "    header = [\"question_count\",\n",
      "               \"exclamation_count\",\n",
      "               \"semicolon_count\",\n",
      "               \"num_techterms\",\n",
      "               \"personal_pronoun\",\n",
      "               \"num_words\",\n",
      "               \"avg_word_length\",\n",
      "               \"num_unique_words\",\n",
      "               \"num_unique_big_words\",\n",
      "               \"num_neg_words\",\n",
      "               \"num_certain_words\",\n",
      "               \"num_hedge_words\",\n",
      "               \"num_sents\",\n",
      "               \"avg_words_sent\",\n",
      "               \"num_paras\",\n",
      "               \"avg_words_para\",\n",
      "               \"count_adjectives\",\n",
      "               \"count_verbs\",\n",
      "               \"count_conjunction\",\n",
      "               \"count_verbs\"\n",
      "               ]\n",
      "    feature_list = []\n",
      "    \n",
      "    feature_list.append(min_max_normalize(punctuation_count(tagged_sents, \"?\")))\n",
      "    feature_list.append(min_max_normalize(punctuation_count(tagged_sents, \"!\")))\n",
      "    feature_list.append(min_max_normalize(punctuation_count(tagged_sents, \";\")))\n",
      "    feature_list.append(min_max_normalize(num_techterms(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_first_person_pronoun_chunk(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_words(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(avg_word_length(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_unique_words(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_unique_big_words(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_neg_words(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_certain_words(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_hedge_words(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_sents(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(avg_words_sent(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_paras(tagged_paras)))\n",
      "    feature_list.append(min_max_normalize(avg_words_para(tagged_paras)))\n",
      "    feature_list.append(min_max_normalize(count_pos(tagged_words, \"JJ\")))\n",
      "    feature_list.append(min_max_normalize(count_pos(tagged_words, \"VB\")))\n",
      "    feature_list.append(min_max_normalize(map(lambda x, y: x + y, count_pos(tagged_words, \"CC\"), count_pos(tagged_words, \"IN\"))))\n",
      "    feature_list.append(min_max_normalize(count_pos(tagged_words, \"VB\")))\n",
      "    df = pandas.DataFrame(feature_list, columns=header)\n",
      "    return df\n",
      "    \n",
      "def funny_matrix():\n",
      "\n",
      "def informative_matrix():\n",
      "    header = [\"question_count\",\n",
      "               \"exclamation_count\",\n",
      "               \"semicolon_count\",\n",
      "               \"num_techterms\",\n",
      "               \"personal_pronoun\",\n",
      "               \"num_words\",\n",
      "               \"avg_word_length\",\n",
      "               \"num_unique_words\",\n",
      "               \"num_unique_big_words\",\n",
      "               \"num_neg_words\",\n",
      "               \"num_certain_words\",\n",
      "               \"num_hedge_words\",\n",
      "               \"num_sents\",\n",
      "               \"avg_words_sent\",\n",
      "               \"num_paras\",\n",
      "               \"avg_words_para\",\n",
      "               \"count_adjectives\",\n",
      "               \"count_verbs\",\n",
      "               \"count_conjunction\",\n",
      "               \"count_verbs\"\n",
      "               ]\n",
      "    feature_list = []\n",
      "    \n",
      "    feature_list.append(min_max_normalize(punctuation_count(tagged_sents, \"?\")))\n",
      "    feature_list.append(min_max_normalize(punctuation_count(tagged_sents, \"!\")))\n",
      "    feature_list.append(min_max_normalize(punctuation_count(tagged_sents, \";\")))\n",
      "    feature_list.append(min_max_normalize(num_techterms(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_first_person_pronoun_chunk(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_words(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(avg_word_length(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_unique_words(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_unique_big_words(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_neg_words(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_certain_words(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_hedge_words(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_sents(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(avg_words_sent(tagged_sents)))\n",
      "    feature_list.append(min_max_normalize(num_paras(tagged_paras)))\n",
      "    feature_list.append(min_max_normalize(avg_words_para(tagged_paras)))\n",
      "    feature_list.append(min_max_normalize(count_pos(tagged_words, \"JJ\")))\n",
      "    feature_list.append(min_max_normalize(count_pos(tagged_words, \"VB\")))\n",
      "    feature_list.append(min_max_normalize(map(lambda x, y: x + y, count_pos(tagged_words, \"CC\"), count_pos(tagged_words, \"IN\"))))\n",
      "    feature_list.append(min_max_normalize(count_pos(tagged_words, \"VB\")))\n",
      "    df = pandas.DataFrame(feature_list, columns=header)\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Split Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Train Test Predict"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Plots"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Discourse"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}