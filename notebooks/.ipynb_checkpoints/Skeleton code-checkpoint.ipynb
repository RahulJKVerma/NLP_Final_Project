{
 "metadata": {
  "name": "",
  "signature": "sha256:b497e81735539b04b92a7b48a1664b62e75a8aef433c480afea6304fd0e8eb39"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Notes\n",
      "1. Get frequent nouns n grams\n",
      "2. Get frequent verb n grams\n",
      "3. Variety in the talk\n",
      "    1. Number of slides - Check for references like (this slide/this video).\n",
      "    2. Laughter \n",
      "    3. More goes here.. \n",
      "\n",
      "## Questions\n",
      "1. Did we lose the paragraph breaks? - Text tiling needs this\n",
      "\n",
      "## Todo\n",
      "1. Would be great to have a method in ted class to extract the metadata."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Classification features\n",
      "1. Punctuations.\n",
      "    1. '?' - thought provoking/call to action\n",
      "    2. '!' - engage audience through expression\n",
      "    3. ';' - could be negative. meaning long sentences\n",
      "2. Collocations:\n",
      "    1. Bigrams or collocated nouns/verbs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import pickle\n",
      "import nltk\n",
      "import string\n",
      "import json, urllib, csv\n",
      "from nltk.corpus import wordnet as wn\n",
      "sys.path.append('../Scripts')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# loading data from pickle files (tokenization & tagging- default nltk)\n",
      "import ted_object\n",
      "from ted_object import ted\n",
      "t_obj = ted()\n",
      "\n",
      "tagged_sents = t_obj.tagged_sents_talk()\n",
      "tagged_words = t_obj.tagged_words_talk()\n",
      "sents = t_obj.sents_talk()\n",
      "words = t_obj.words_talk()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Category 1: Confusing, 2: Funny, 3: Informative\n",
      "# We said we would have 3 binary classifiers\n",
      "def read_category_labels():\n",
      "    rating_file = open('../Corpus/User Reviews.csv', 'rb')\n",
      "    count = 0\n",
      "    categ_list = list()\n",
      "    for line in rating_file:\n",
      "        item = line.split(',')\n",
      "        count += 1\n",
      "        if count > 108: # adding this till the last batch of categories are filled out\n",
      "            break\n",
      "#         categ = int(item[15])*1 + int(item[16])*2 + int(item[17]) * 3\n",
      "        categ_list.append([item[15], item[16], item[17]])\n",
      "    return categ_list\n",
      "    #     print count, [item[15], item[16], item[17]], categ\n",
      "    # categ_list\n",
      "    \n",
      "# Here's category labels for regression (percentages instead of [0 or 1])\n",
      "all_labels = pickle.load(open('../Corpus/y.p', 'rb'))\n",
      "confusing_y = [row[0] for row in all_labels]\n",
      "funny_y = [row[1] for row in all_labels]\n",
      "informative_y = [row[2] for row in all_labels]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Clean up text: Stop word removal & Stemming - Code here if we need to revisit the current stemming"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import *\n",
      "from nltk.stem.porter import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenize_text(text):\n",
      "    pattern =[\"(?x)([A-Z]\\.)+\",\n",
      "               \"\\w+([-]\\w+)*\",\n",
      "               \"\\$?\\d+(\\.\\d+)?%?\",\n",
      "               \"\\.\\.\\.\",\n",
      "               \"[.,?;]+\"]\n",
      "    pattern = \"|\".join(pattern)\n",
      "    text = \" \".join(text) if isinstance(text, list) == True else text\n",
      "    tokens = nltk.regexp_tokenize(text,pattern)\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wnl = WordNetLemmatizer()\n",
      "\n",
      "# reduce words to root format\n",
      "def get_root_phrase(word, tag):\n",
      "    word = word.lower()\n",
      "    #trim joint words\n",
      "    if word.endswith(\"'s\"):\n",
      "        print word\n",
      "        word = word[:-2]\n",
      "    #don't cut short words. e.g. rss would be reduced to r\n",
      "    if len(word)<=3 and word not in COMMON_WORDS:\n",
      "        return word\n",
      "    #try to morphy\n",
      "    morphy = wn.morphy(word)\n",
      "    if morphy is None:\n",
      "        morphy = word\n",
      "    #lemmatize morphy\n",
      "    lem = wnl.lemmatize(morphy, 'n')\n",
      "    if lem and len(word)>3: \n",
      "        morphy = lem\n",
      "    return morphy\n",
      "\n",
      "# Get morphy words in a sentence\n",
      "def get_morphys(tagged_sentence):\n",
      "   return [ (get_root_phrase(word, tag), tag) for (word, tag) in tagged_sentence \\\n",
      "               if word not in stop_words and (word.isalnum() or '-' in word) and not word.isdigit()\\\n",
      "               and len(get_root_phrase(word, tag))>1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#remove all stop words and punctuations\n",
      "non_stop_words = list()\n",
      "for talk in tagged_words:\n",
      "    non_stop_words.append([(w,t) for w,t in talk if w.lower() not in stopwords.words('english') and\n",
      "                       w.lower() not in string.punctuation])\n",
      "pickle.dump(non_stop_words, open( \"../Corpus/non_stop_tagged_words_talks.p\", \"wb\" ))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "porter_stemmer = PorterStemmer()\n",
      "stemmed_words = list()\n",
      "for talk in non_stop_words:\n",
      "    stemmed_words.append([(porter_stemmer.stem(w),t) for w,t in talk])\n",
      "pickle.dump(stemmed_words, open( \"../Corpus/stemmed_tagged_words_talks.p\", \"wb\" ))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Helper Functions for Chunking"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#helper function to chunk the sentenses based on a grammar\n",
      "def parse_sents(ip_sent, grammar):\n",
      "    cp = nltk.RegexpParser(grammar)\n",
      "    result = cp.parse(ip_sent)\n",
      "    return result\n",
      "\n",
      "#helper function to extract subtrees\n",
      "def extract_tree(sents,grammar, match):\n",
      "    ret_list = []\n",
      "    for sent in sents:\n",
      "        res = parse_sents(sent, grammar)\n",
      "        ret_list += [' '.join([word for(word, tag) in subtree]) for subtree in res.subtrees(filter = lambda t: t.node == match)]\n",
      "    return ret_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Naive Bayes Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "category_list = read_category_labels()\n",
      "if len(category_list) != 108: #len(tagged_sents):\n",
      "    print \"ERROR\"\n",
      "confusing_set = list()\n",
      "funny_set = list()\n",
      "info_set = list()\n",
      "for i in range(len(category_list)):\n",
      "    confusing_set.append((category_list[i][0], tagged_sents[i]))\n",
      "    funny_set.append((category_list[i][1], tagged_sents[i]))\n",
      "    info_set.append((category_list[i][2], tagged_sents[i]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: All number features should be bucketed to a range. ##\n",
      "BUCKET_RANGE = 10    # how about min-max normalization (range 0-1) ?\n",
      "\n",
      "def min_max_normalize(feature):\n",
      "    '''Converts a list of numberical values to a list of values between 0 and 1'''\n",
      "    minVal = min(feature)\n",
      "    maxVal = max(feature)\n",
      "    normalized = [((e - minVal / (maxVal - minVal)) for e in feature]\n",
      "    return normalized\n",
      "    \n",
      "def avg_sent_len(talk_sents):\n",
      "    num_sents = len(talk_sents)\n",
      "    total_len = sum([len(sent) for sent in talk_sents])\n",
      "    avg_len = (total_len)/(num_sents * BUCKET_RANGE)\n",
      "    return [('AVGLEN_%s' %(avg_len), 1)]\n",
      "\n",
      "def punctuation_count(talk_sents, punc):\n",
      "    num_punc = sum([1 for sent in talk_sents for w,t in sent if punc == w])\n",
      "    return [('%s_%s' %(punc,num_punc/BUCKET_RANGE), 1)]\n",
      "\n",
      "# From the paper Rahul mentioned for Technical terms - Citation required\n",
      "def num_techterms(talk_sents):\n",
      "    njn_list = extract_tree(talk_sents, 'NJN: {(<N.*>*<J*>*<N.*>+)+}','NJN')\n",
      "    npn_list = extract_tree(talk_sents, 'NPN: {(<N.*>+<P.*><N.*>+)+}','NPN')\n",
      "    return [('NUMTECH_%s' %((len(njn_list) + len(npn_list))/BUCKET_RANGE), 1)]\n",
      "\n",
      "def num_first_person_pronoun_chunk(talk_sents):\n",
      "    pro_verb = \"(<PR.*>|<N.*P>+)+<TO>?<MD>?<TO>?<VB.*>+\"\n",
      "    VB_grammar = \"PROVB: {\" + pro_verb + \"}\"\n",
      "    pvb_list = extract_tree(talk_sents,VB_grammar, \"PROVB\")\n",
      "    return [('NUMFPRO_%s' %(len(pvb_list)/BUCKET_RANGE), 1)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Possible Features:\n",
      "- **Confusing** and **Informative**\n",
      "    - total number of words\n",
      "    - average word length\n",
      "    - vocabulary richness (total unique words / N)\n",
      "    - number of words longer than 10 characters\n",
      "    - number of sentences\n",
      "    - number of paragraphs\n",
      "    - average words per sentence\n",
      "    - average words per paragraph\n",
      "    - number of adjectives\n",
      "    - number of uncertainty verbs (wonder, consider, suppose)\n",
      "    - negations (no, not, never)\n",
      "    - certainty (always, never)\n",
      "    - hedges (well, kind of, sort of, possibly, maybe)\n",
      "    - number of '?'\n",
      "    - number of '!'\n",
      "    - number of technical terms\n",
      "    - average pronouns per paragraph\n",
      "    - number of conjunctions\n",
      "    - number of prepositions\n",
      "\n",
      "- **Funny**\n",
      "    - number of interjections (wow, yeah)\n",
      "    - intensive adverbs (really, very, quite, special)\n",
      "    - positive emotion (love, nice, sweet)\n",
      "    - negative emotion (hurt, ugly, nasty)\n",
      "    - number of annotated laughs in transcript\n",
      "    - number of adjectives\n",
      "    - number of verbs\n",
      "    - number of '?'\n",
      "    - number of '!'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def con_inf_features(talk_sents):\n",
      "    feature_list = list()\n",
      "    feature_list.extend(avg_sent_len(talk_sents))\n",
      "    feature_list.extend(punctuation_count(talk_sents, '!'))\n",
      "    feature_list.extend(punctuation_count(talk_sents, '?'))\n",
      "    feature_list.extend(punctuation_count(talk_sents, ';'))\n",
      "    feature_list.extend(num_techterms(talk_sents))\n",
      "    feature_list.extend(num_first_person_pronoun_chunk(talk_sents))\n",
      "    feature_set = dict(feature_list)\n",
      "    return feature_set\n",
      "\n",
      "def funny_features(talk_sents):\n",
      "    feature_list = list()\n",
      "    featrue_list.extend()\n",
      "    return "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Category information should be encoded. For now everything is 1\n",
      "def create_feature_sets(feature_function, dataset, split=[.7,.2,.1]):\n",
      "    featureset = [ (feature_function(talk_sents), categ) for (categ, talk_sents) in dataset ]\n",
      "    trainlen = int(split[0] * len(featureset))\n",
      "    devlen = int(split[1] * len(featureset))\n",
      "    return featureset[:trainlen], featureset[trainlen:trainlen+devlen], featureset[trainlen:trainlen+devlen:]\n",
      "#     return featureset[:int(len(featureset)*split)], featureset[int(len(featureset)*split):]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set_con, dev_set_con, test_set_con = create_feature_sets(create_features, confusing_set)\n",
      "cl_con = nltk.NaiveBayesClassifier.train(train_set_con)\n",
      "print \"%.3f\" % nltk.classify.accuracy(cl_con, dev_set_con)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.619\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set_fun, dev_set_fun, test_set_fun = create_feature_sets(create_features, funny_set)\n",
      "cl_fun = nltk.NaiveBayesClassifier.train(train_set_fun)\n",
      "print \"%.3f\" % nltk.classify.accuracy(cl_fun, dev_set_fun)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.524\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set_info, dev_set_info, test_set_info = create_feature_sets(create_features, info_set)\n",
      "cl_info = nltk.NaiveBayesClassifier.train(train_set_info)\n",
      "print \"%.3f\" % nltk.classify.accuracy(cl_info, dev_set_info)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.333\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Good for testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl_con.show_most_informative_features(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl_fun.show_most_informative_features(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl_info.show_most_informative_features(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_set = create_feature_sets(create_features, confusing_set, [1,0,0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_kfolds(dataset, k=10):\n",
      "    s = len(dataset)/k\n",
      "    return [ (dataset[:i*s]+dataset[(i+1)*s:], dataset[i*s:(i+1)*s]) for i in range(0, k)]\n",
      "\n",
      "\n",
      "def cross_validate(feature_set, k=10, classifier = nltk.NaiveBayesClassifier):\n",
      "    feature_sets = create_kfolds(feature_set, k)\n",
      "    accuracy = 0.0\n",
      "    index = 0\n",
      "    errors = []\n",
      "    for (train_set, dev_set) in feature_sets:\n",
      "        cl = classifier.train(train_set)\n",
      "        acc = nltk.classify.accuracy(cl, dev_set)\n",
      "        errors += [ (i, int(dev_set[i][1]), int(cl.classify(dev_set[i][0])), dev_set[i][0]) for i in range(len(dev_set))\n",
      "                   if cl.classify(dev_set[i][0])!=dev_set[i][1]]\n",
      "        accuracy+=acc\n",
      "        print \"%d\\t%.3f\" % (index, acc)\n",
      "        index +=1\n",
      "    accuracy /= k\n",
      "    print \"Cross Validation Accuracy = %.3f\" % accuracy\n",
      "    return accuracy, errors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Default is Naive Bayes classifier\n",
      "accuracy, errors = cross_validate(feature_set[0], k=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\t0.200\n",
        "1\t0.600\n",
        "2\t1.000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3\t0.400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4\t0.600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5\t0.400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6\t0.600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7\t0.600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8\t0.600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9\t0.400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10\t0.600\n",
        "11\t0.600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "12\t0.600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "13\t0.600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "14\t0.200\n",
        "15\t0.600\n",
        "16\t0.400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17\t0.600\n",
        "18\t0.400\n",
        "19\t0.400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross Validation Accuracy = 0.520\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Discourse Analysis Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tt = nltk.tokenize.texttiling.TextTilingTokenizer()\n",
      "text = ' '.join([' '.join(words) for words in sents[0]])\n",
      "print type(text)\n",
      "# s,ss,d,b = tt.tokenize(text)\n",
      "# misses paragraph breaks..!!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import pylab\n",
      "# pylab.xlabel(\"Sentence Gap index\")\n",
      "# pylab.ylabel(\"Gap Scores\")\n",
      "# pylab.plot(range(len(s)), s, label=\"Gap Scores\")\n",
      "# pylab.plot(range(len(ss)), ss, label=\"Smoothed Gap scores\")\n",
      "# pylab.plot(range(len(d)), d, label=\"Depth scores\")\n",
      "# pylab.stem(range(len(b)),b)\n",
      "# pylab.legend()\n",
      "# pylab.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Code just put in - delete later if not required"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Might be useful if the Porter Stemmer distorts the words a lot"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import RegexpStemmer\n",
      "#sample for testing the regex stemmer\n",
      "sample_words = \"Helloss Gullivers masters countries yahoos\"\n",
      "my_stemmer = RegexpStemmer('s+$|es$')\n",
      "[my_stemmer.stem(word) for word in sample_words.split()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# helper functions for unigrams, bigrams\n",
      "def print_important_nouns(fd, imp_words):\n",
      "    imp_nouns = [((word, count),fd.freq((word,tag))) for ((word, tag), count) in imp_words if re.match(r'N.*', tag)]\n",
      "    top_nouns = imp_nouns[:15]\n",
      "    print '%-16s' % \"NOUNS\", '%-16s' % \"COUNT\", '%-16s' % \"FREQUENCY\\n\"\n",
      "    for ((noun, count), freq) in top_nouns:\n",
      "        print '%-16s' % noun, '%-16s' % count, \"%-16s\" % round((freq * 100), 4)\n",
      "    return imp_nouns\n",
      "\n",
      "def print_important_verbs(fd, imp_words):\n",
      "    imp_verbs = [((word, count),fd.freq((word,tag))) for ((word, tag), count) in imp_words if re.match(r'V+', tag)]\n",
      "    print '%-16s' % \"VERBS\", '%-16s' % \"COUNT\", '%-16s' % \"FREQUENCY\\n\"\n",
      "    top_verbs = imp_verbs[5:20]\n",
      "    for ((verb, count), freq) in top_verbs:\n",
      "        print '%-16s' % verb, '%-16s' % count, \"%-16s\" % round((freq * 100), 4)\n",
      "    return imp_verbs\n",
      "\n",
      "def print_bigrams(stemmed_words):\n",
      "    bigram_fd = nltk.FreqDist(nltk.bigrams([w for w,t in stemmed_words if re.match(r'(JJ|N.*)', t)]))\n",
      "    bigram_fd_items = bigram_fd.items()\n",
      "    bigram_fd_nouns = [(' '.join(map(str,(w1,w2))), c, bigram_fd.freq((w1,w2)) )for ((w1,w2), c) in bigram_fd_items \n",
      "                   if w1 not in string.punctuation and \n",
      "                     w2 not in string.punctuation]\n",
      "\n",
      "    bigram_nouns = [noun for (noun, count) in bigram_fd_items]\n",
      "\n",
      "    print '%-25s' % \"BIGRAM NOUNS\", '%-25s' % \"COUNT\", '%-25s' % \"FREQUENCY\\n\"\n",
      "    for (noun, count, freq) in bigram_fd_nouns[:20]:\n",
      "        print '%-25s' % noun, '%-25s' % count, \"%-25s\" % round((freq * 100), 4)\n",
      "    return bigram_nouns\n",
      "\n",
      "def print_ngram_nouns(stemmed_words):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd_w = nltk.FreqDist(tagged_words[0])\n",
      "imp_nouns = print_important_nouns(fd_w, fd_w.items()[:200])\n",
      "imp_verbs = print_important_verbs(fd_w, fd_w.items()[:200])\n",
      "imp_bigrams = print_bigrams(tagged_words[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Colocations\n",
      "def get_bigram_collocations(tag_words):\n",
      "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "    coloc_words = [w for w,t in tag_words if re.match(r'[a-z]+', w)]\n",
      "    finder = BigramCollocationFinder.from_words(coloc_words)\n",
      "    return finder.nbest(bigram_measures.pmi, 20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}