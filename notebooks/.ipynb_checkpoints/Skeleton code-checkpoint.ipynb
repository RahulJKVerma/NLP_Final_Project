{
 "metadata": {
  "name": "",
  "signature": "sha256:dfd245c453013f7c9920e4c99b7503d5b567e9295f2b27b1c80b2c186a1c722d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Notes\n",
      "1. Get frequent nouns n grams\n",
      "2. Get frequent verb n grams\n",
      "3. Variety in the talk\n",
      "    1. Number of slides - Check for references like (this slide/this video).\n",
      "    2. Laughter \n",
      "    3. More goes here.. \n",
      "\n",
      "## Questions\n",
      "1. Did we lose the paragraph breaks? - Text tiling needs this\n",
      "\n",
      "## Todo\n",
      "1. Would be great to have a method in ted class to extract the metadata."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Classification features\n",
      "1. Punctuations.\n",
      "    1. '?' - thought provoking/call to action\n",
      "    2. '!' - engage audience through expression\n",
      "    3. ';' - could be negative. meaning long sentences\n",
      "2. Collocations:\n",
      "    1. Bigrams or collocated nouns/verbs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import pickle\n",
      "import nltk\n",
      "import string\n",
      "import json, urllib, csv\n",
      "from nltk.corpus import wordnet as wn\n",
      "sys.path.append('../Scripts')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# loading data from pickle files (tokenization & tagging- default nltk)\n",
      "import ted_object\n",
      "from ted_object import ted\n",
      "t_obj = ted()\n",
      "\n",
      "tagged_sents = t_obj.tagged_sents_talk()\n",
      "tagged_words = t_obj.tagged_words_talk()\n",
      "sents = t_obj.sents_talk()\n",
      "words = t_obj.words_talk()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_text(jobject, urls):\n",
      "    texts = []\n",
      "    for talk in jobject['results']:\n",
      "        if talk['url'] not in urls:\n",
      "            #print talk['url']\n",
      "            text = []\n",
      "            for line in talk['content']:\n",
      "                text.append(line['words']['text'].encode('ascii', 'ignore'))\n",
      "            blob = ' '.join(text)\n",
      "            texts.append(blob)\n",
      "            urls.append(talk['url'])\n",
      "    return texts, urls\n",
      "\n",
      "def get_metadata(jobject):\n",
      "    meta = []\n",
      "    for talk in jobject['results']['collection1']:\n",
      "        meta.append((talk['title'].encode('ascii', 'ignore'), \n",
      "            talk['speaker'].encode('ascii', 'ignore')[:-1], \n",
      "            talk['nviews'].encode('ascii', 'ignore'), \n",
      "            talk['topics'].encode('ascii', 'ignore')))\n",
      "    return meta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "api_call = \"https://www.kimonolabs.com/api/92nwvy72?apikey=VP6IONzhcPRy5y0C3OVQ6o07Kp85v6vC&kimbypage=1&kimoffset=\"\n",
      "total_list = []\n",
      "urls = []\n",
      "amount = 0 \n",
      "while amount <= 50000:\n",
      "    texts, urls = get_text(json.load(urllib.urlopen(api_call+str(amount))), urls)\n",
      "    total_list.extend(texts)\n",
      "    amount += 2500\n",
      "\n",
      "print \"Created a list of talks!\"\n",
      "print \"Length = %d\" %len(total_list) # len of total_list is 177"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Created a list of talks!\n",
        "Length = 0\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Temporarily using the text file\n",
      "raw_text = open('../Corpus/raw_text/ted.txt')\n",
      "talks = list()\n",
      "for text in raw_text:\n",
      "    talks.append(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Clean up text: Stop word removal & Stemming"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import *\n",
      "from nltk.stem.porter import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenize_text(text):\n",
      "    pattern =[\"(?x)([A-Z]\\.)+\",\n",
      "               \"\\w+([-]\\w+)*\",\n",
      "               \"\\$?\\d+(\\.\\d+)?%?\",\n",
      "               \"\\.\\.\\.\",\n",
      "               \"[.,?;]+\"]\n",
      "    pattern = \"|\".join(pattern)\n",
      "    text = \" \".join(text) if isinstance(text, list) == True else text\n",
      "    tokens = nltk.regexp_tokenize(text,pattern)\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wnl = WordNetLemmatizer()\n",
      "\n",
      "# reduce words to root format\n",
      "def get_root_phrase(word, tag):\n",
      "    word = word.lower()\n",
      "    #trim joint words\n",
      "    if word.endswith(\"'s\"):\n",
      "        print word\n",
      "        word = word[:-2]\n",
      "    #don't cut short words. e.g. rss would be reduced to r\n",
      "    if len(word)<=3 and word not in COMMON_WORDS:\n",
      "        return word\n",
      "    #try to morphy\n",
      "    morphy = wn.morphy(word)\n",
      "    if morphy is None:\n",
      "        morphy = word\n",
      "    #lemmatize morphy\n",
      "    lem = wnl.lemmatize(morphy, 'n')\n",
      "    if lem and len(word)>3: \n",
      "        morphy = lem\n",
      "    return morphy\n",
      "\n",
      "# Get morphy words in a sentence\n",
      "def get_morphys(tagged_sentence):\n",
      "   return [ (get_root_phrase(word, tag), tag) for (word, tag) in tagged_sentence \\\n",
      "               if word not in stop_words and (word.isalnum() or '-' in word) and not word.isdigit()\\\n",
      "               and len(get_root_phrase(word, tag))>1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#remove all stop words and punctuations\n",
      "non_stop_words = list()\n",
      "for talk in tagged_words:\n",
      "    non_stop_words.append([(w,t) for w,t in talk if w.lower() not in stopwords.words('english') and\n",
      "                       w.lower() not in string.punctuation])\n",
      "pickle.dump(non_stop_words, open( \"../Corpus/non_stop_tagged_words_talks.p\", \"wb\" ))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "porter_stemmer = PorterStemmer()\n",
      "stemmed_words = list()\n",
      "for talk in non_stop_words:\n",
      "    stemmed_words.append([(porter_stemmer.stem(w),t) for w,t in talk])\n",
      "pickle.dump(stemmed_words, open( \"../Corpus/stemmed_tagged_words_talks.p\", \"wb\" ))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Might be useful if the Porter Stemmer distorts the words a lot"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import RegexpStemmer\n",
      "#sample for testing the regex stemmer\n",
      "sample_words = \"Helloss Gullivers masters countries yahoos\"\n",
      "my_stemmer = RegexpStemmer('s+$|es$')\n",
      "[my_stemmer.stem(word) for word in sample_words.split()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmed_words[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Helper Functions for Keyphrase related features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#helper function to chunk the sentenses based on a grammar\n",
      "def parse_sents(ip_sent, grammar):\n",
      "    cp = nltk.RegexpParser(grammar)\n",
      "    result = cp.parse(ip_sent)\n",
      "    return result\n",
      "\n",
      "#helper function to extract subtrees\n",
      "def extract_tree(sents,grammar, match):\n",
      "    ret_list = []\n",
      "    for sent in sents:\n",
      "        res = parse_sents(sent, grammar)\n",
      "        ret_list += [' '.join([word for(word, tag) in subtree]) for subtree in res.subtrees(filter = lambda t: t.node == match)]\n",
      "    return ret_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# helper functions for unigrams, bigrams\n",
      "def print_important_nouns(fd, imp_words):\n",
      "    imp_nouns = [((word, count),fd.freq((word,tag))) for ((word, tag), count) in imp_words if re.match(r'N.*', tag)]\n",
      "    top_nouns = imp_nouns[:15]\n",
      "    print '%-16s' % \"NOUNS\", '%-16s' % \"COUNT\", '%-16s' % \"FREQUENCY\\n\"\n",
      "    for ((noun, count), freq) in top_nouns:\n",
      "        print '%-16s' % noun, '%-16s' % count, \"%-16s\" % round((freq * 100), 4)\n",
      "    return imp_nouns\n",
      "\n",
      "def print_important_verbs(fd, imp_words):\n",
      "    imp_verbs = [((word, count),fd.freq((word,tag))) for ((word, tag), count) in imp_words if re.match(r'V+', tag)]\n",
      "    print '%-16s' % \"VERBS\", '%-16s' % \"COUNT\", '%-16s' % \"FREQUENCY\\n\"\n",
      "    top_verbs = imp_verbs[5:20]\n",
      "    for ((verb, count), freq) in top_verbs:\n",
      "        print '%-16s' % verb, '%-16s' % count, \"%-16s\" % round((freq * 100), 4)\n",
      "    return imp_verbs\n",
      "\n",
      "def print_bigrams(stemmed_words):\n",
      "    bigram_fd = nltk.FreqDist(nltk.bigrams([w for w,t in stemmed_words if re.match(r'(JJ|N.*)', t)]))\n",
      "    bigram_fd_items = bigram_fd.items()\n",
      "    bigram_fd_nouns = [(' '.join(map(str,(w1,w2))), c, bigram_fd.freq((w1,w2)) )for ((w1,w2), c) in bigram_fd_items \n",
      "                   if w1 not in string.punctuation and \n",
      "                     w2 not in string.punctuation]\n",
      "\n",
      "    bigram_nouns = [noun for (noun, count) in bigram_fd_items]\n",
      "\n",
      "    print '%-25s' % \"BIGRAM NOUNS\", '%-25s' % \"COUNT\", '%-25s' % \"FREQUENCY\\n\"\n",
      "    for (noun, count, freq) in bigram_fd_nouns[:20]:\n",
      "        print '%-25s' % noun, '%-25s' % count, \"%-25s\" % round((freq * 100), 4)\n",
      "    return bigram_nouns\n",
      "\n",
      "def print_ngram_nouns(stemmed_words):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Colocations\n",
      "def get_bigram_collocations(tag_words):\n",
      "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "    coloc_words = [w for w,t in tag_words if re.match(r'[a-z]+', w)]\n",
      "    finder = BigramCollocationFinder.from_words(coloc_words)\n",
      "    return finder.nbest(bigram_measures.pmi, 20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Naive Bayes Classifier for keyphrase related features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def avg_sent_len(talk_sents):\n",
      "    num_sents = len(talk_sents)\n",
      "    total_len = sum([len(sent) for sent in talk_sents])\n",
      "    print num_sents, total_len\n",
      "    return [('AVGLEN_%s' %(float(total_len/num_sents)), 1)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_features(talk_sents\n",
      "                    \n",
      "    feature_set = dict()    \n",
      "    return feature_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_feature_sets(feature_function, dataset, split=.9):\n",
      "    print type(dataset), type(dataset[0])\n",
      "    featureset = [ (feature_function(talk_sents), 1) for talk_sents  in dataset ]\n",
      "    return featureset[:int(len(featureset)*split)], featureset[int(len(featureset)*split):]\n",
      "\n",
      "train_set, dev_set = create_feature_sets(create_features, sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'list'> <type 'list'>\n",
        "254 3998\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "dictionary update sequence element #0 has length 11; 2 is required",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-50-5fe3cefbf7dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_feature_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-50-5fe3cefbf7dd>\u001b[0m in \u001b[0;36mcreate_feature_sets\u001b[1;34m(feature_function, dataset, split)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_feature_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfeatureset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfeature_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtalk_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtalk_sents\u001b[0m  \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-49-3c531a364ddd>\u001b[0m in \u001b[0;36mcreate_features\u001b[1;34m(talk_sents)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtalk_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mfeature_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_sent_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtalk_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeature_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 11; 2 is required"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl = nltk.NaiveBayesClassifier.train(train_set)\n",
      "print \"%.3f\" % nltk.classify.accuracy(cl, dev_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Good for testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl.show_most_informative_features(100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_set = create_feature_sets(create_features, stemmed_words, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_kfolds(dataset, k=10):\n",
      "    s = len(dataset)/k\n",
      "    return [ (dataset[:i*s]+dataset[(i+1)*s:], dataset[i*s:(i+1)*s]) for i in range(0, k)]\n",
      "\n",
      "\n",
      "def cross_validate(feature_set, k=10, classifier = nltk.NaiveBayesClassifier):\n",
      "    feature_sets = create_kfolds(feature_set, k)\n",
      "    accuracy = 0.0\n",
      "    index = 0\n",
      "    errors = []\n",
      "    for (train_set, dev_set) in feature_sets:\n",
      "        cl = classifier.train(train_set)\n",
      "        acc = nltk.classify.accuracy(cl, dev_set)\n",
      "        errors += [ (index, i, categories[int(dev_set[i][1])], categories[int(cl.classify(dev_set[i][0]))], dev_set[i][0]) \n",
      "                   for i in range(len(dev_set))\n",
      "                   if cl.classify(dev_set[i][0])!=dev_set[i][1]]\n",
      "        accuracy+=acc\n",
      "        print \"%d\\t%.3f\" % (index, acc)\n",
      "        index +=1\n",
      "    accuracy /= k\n",
      "    print \"Cross Validation Accuracy = %.3f\" % accuracy\n",
      "    return accuracy, errors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Default is Naive Bayes classifier\n",
      "accuracy, errors = cross_validate(feature_set[0], k=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Exploration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd_w = nltk.FreqDist(tagged_words[0])\n",
      "imp_nouns = print_important_nouns(fd_w, fd_w.items()[:200])\n",
      "imp_verbs = print_important_verbs(fd_w, fd_w.items()[:200])\n",
      "imp_bigrams = print_bigrams(tagged_words[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NOUNS            COUNT            FREQUENCY\n",
        "      \n",
        ")                23               0.5753          \n",
        "Laughter         22               0.5503          \n",
        "education        22               0.5503          \n",
        "(                16               0.4002          \n",
        "people           13               0.3252          \n",
        "way              11               0.2751          \n",
        "things           10               0.2501          \n",
        "system           8                0.2001          \n",
        "children         7                0.1751          \n",
        "school           7                0.1751          \n",
        "world            7                0.1751          \n",
        "intelligence     6                0.1501          \n",
        "kids             6                0.1501          \n",
        "life             6                0.1501          \n",
        "thing            6                0.1501          \n",
        "VERBS            COUNT            FREQUENCY\n",
        "      \n",
        "are              20               0.5003          \n",
        "'re              19               0.4752          \n",
        "be               19               0.4752          \n",
        "have             19               0.4752          \n",
        "did              16               0.4002          \n",
        "do               14               0.3502          \n",
        "had              11               0.2751          \n",
        "know             11               0.2751          \n",
        "were             11               0.2751          \n",
        "being            9                0.2251          \n",
        "have             8                0.2001          \n",
        "'ve              7                0.1751          \n",
        "went             7                0.1751          \n",
        "'m               6                0.1501          \n",
        "been             6                0.1501          \n",
        "BIGRAM NOUNS             "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " COUNT                     FREQUENCY\n",
        "               \n",
        "Royal Ballet              4                         0.444                    \n",
        "education system          4                         0.444                    \n",
        "interest education        4                         0.444                    \n",
        "Los Angeles               3                         0.333                    \n",
        "public education          3                         0.333                    \n",
        "Ballet School             2                         0.222                    \n",
        "Gillian Lynne             2                         0.222                    \n",
        "Stratford Los             2                         0.222                    \n",
        "academic ability          2                         0.222                    \n",
        "creative capacities       2                         0.222                    \n",
        "drawing lesson            2                         0.222                    \n",
        "earth years               2                         0.222                    \n",
        "education world           2                         0.222                    \n",
        "form life                 2                         0.222                    \n",
        "heads side                2                         0.222                    \n",
        "room mother               2                         0.222                    \n",
        "university professors     2                         0.222                    \n",
        "view intelligence         2                         0.222                    \n",
        "whole system              2                         0.222                    \n",
        "'30s parents              1                         0.111                    \n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Discourse Analysis Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tt = nltk.tokenize.texttiling.TextTilingTokenizer()\n",
      "text = ' '.join([' '.join(words) for words in sents[0]])\n",
      "print type(text)\n",
      "# s,ss,d,b = tt.tokenize(text)\n",
      "# misses paragraph breaks..!!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'str'>\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import pylab\n",
      "# pylab.xlabel(\"Sentence Gap index\")\n",
      "# pylab.ylabel(\"Gap Scores\")\n",
      "# pylab.plot(range(len(s)), s, label=\"Gap Scores\")\n",
      "# pylab.plot(range(len(ss)), ss, label=\"Smoothed Gap scores\")\n",
      "# pylab.plot(range(len(d)), d, label=\"Depth scores\")\n",
      "# pylab.stem(range(len(b)),b)\n",
      "# pylab.legend()\n",
      "# pylab.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import wordnet as wn\n",
      "import re\n",
      "w_mphy = [wn.morphy(w,wn.NOUN) for w,t in tagged_words[0] if re.match('N*',t)]\n",
      "fd_wm = nltk.FreqDist(w_mphy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd_wm.items()[:10]\n",
      "# tagged_words[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "[(None, 2648),\n",
        " ('a', 74),\n",
        " ('in', 53),\n",
        " ('it', 47),\n",
        " ('wa', 46),\n",
        " ('think', 28),\n",
        " ('have', 27),\n",
        " ('education', 22),\n",
        " ('are', 20),\n",
        " ('at', 20)]"
       ]
      }
     ],
     "prompt_number": 35
    }
   ],
   "metadata": {}
  }
 ]
}