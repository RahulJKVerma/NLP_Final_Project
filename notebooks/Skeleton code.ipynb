{
 "metadata": {
  "name": "",
  "signature": "sha256:c1af4a66166bf3e19059ef4d394479900607376ced49e910893a7d2a046d41ba"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Notes\n",
      "1. Get frequent nouns n grams\n",
      "2. Get frequent verb n grams\n",
      "3. Variety in the talk\n",
      "    1. Number of slides - Check for references like (this slide/this video).\n",
      "    2. Laughter \n",
      "    3. More goes here.. \n",
      "\n",
      "## Questions\n",
      "1. Did we lose the paragraph breaks? - Text tiling needs this\n",
      "\n",
      "## Todo\n",
      "1. Would be great to have a method in ted class to extract the metadata."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Classification features\n",
      "1. Punctuations.\n",
      "    1. '?' - thought provoking/call to action\n",
      "    2. '!' - engage audience through expression\n",
      "    3. ';' - could be negative. meaning long sentences\n",
      "2. Collocations:\n",
      "    1. Bigrams or collocated nouns/verbs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import pickle\n",
      "import nltk\n",
      "import string\n",
      "import json, urllib, csv\n",
      "from nltk.corpus import wordnet as wn\n",
      "sys.path.append('../Scripts')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# loading data from pickle files (tokenization & tagging- default nltk)\n",
      "import ted_object\n",
      "from ted_object import ted\n",
      "t_obj = ted()\n",
      "\n",
      "tagged_sents = t_obj.tagged_sents_talk()\n",
      "tagged_words = t_obj.tagged_words_talk()\n",
      "sents = t_obj.sents_talk()\n",
      "words = t_obj.words_talk()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_text(jobject, urls):\n",
      "    texts = []\n",
      "    for talk in jobject['results']:\n",
      "        if talk['url'] not in urls:\n",
      "            #print talk['url']\n",
      "            text = []\n",
      "            for line in talk['content']:\n",
      "                text.append(line['words']['text'].encode('ascii', 'ignore'))\n",
      "            blob = ' '.join(text)\n",
      "            texts.append(blob)\n",
      "            urls.append(talk['url'])\n",
      "    return texts, urls\n",
      "\n",
      "def get_metadata(jobject):\n",
      "    meta = []\n",
      "    for talk in jobject['results']['collection1']:\n",
      "        meta.append((talk['title'].encode('ascii', 'ignore'), \n",
      "            talk['speaker'].encode('ascii', 'ignore')[:-1], \n",
      "            talk['nviews'].encode('ascii', 'ignore'), \n",
      "            talk['topics'].encode('ascii', 'ignore')))\n",
      "    return meta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "api_call = \"https://www.kimonolabs.com/api/92nwvy72?apikey=VP6IONzhcPRy5y0C3OVQ6o07Kp85v6vC&kimbypage=1&kimoffset=\"\n",
      "total_list = []\n",
      "urls = []\n",
      "amount = 0 \n",
      "while amount <= 50000:\n",
      "    texts, urls = get_text(json.load(urllib.urlopen(api_call+str(amount))), urls)\n",
      "    total_list.extend(texts)\n",
      "    amount += 2500\n",
      "\n",
      "print \"Created a list of talks!\"\n",
      "print \"Length = %d\" %len(total_list) # len of total_list is 177"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Created a list of talks!\n",
        "Length = 0\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Clean up text: Stop word removal & Stemming"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import *\n",
      "from nltk.stem.porter import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenize_text(text):\n",
      "    pattern =[\"(?x)([A-Z]\\.)+\",\n",
      "               \"\\w+([-]\\w+)*\",\n",
      "               \"\\$?\\d+(\\.\\d+)?%?\",\n",
      "               \"\\.\\.\\.\",\n",
      "               \"[.,?;]+\"]\n",
      "    pattern = \"|\".join(pattern)\n",
      "    text = \" \".join(text) if isinstance(text, list) == True else text\n",
      "    tokens = nltk.regexp_tokenize(text,pattern)\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = tokenize_text(q)\n",
      "tagged_sentence=tagger.tag(tokens)\n",
      "morphys = get_morphys(tagged_sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#remove all stop words and punctuations\n",
      "non_stop_words = list()\n",
      "for talk in tagged_words:\n",
      "    non_stop_words.append([(w,t) for w,t in talk if w.lower() not in stopwords.words('english') and\n",
      "                       w.lower() not in string.punctuation])\n",
      "pickle.dump(non_stop_words, open( \"../Corpus/non_stop_tagged_words_talks.p\", \"wb\" ))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "porter_stemmer = PorterStemmer()\n",
      "stemmed_words = list()\n",
      "for talk in non_stop_words:\n",
      "    stemmed_words.append([(porter_stemmer.stem(w),t) for w,t in talk])\n",
      "pickle.dump(stemmed_words, open( \"../Corpus/stemmed_tagged_words_talks.p\", \"wb\" ))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Might be useful if the Porter Stemmer distorts the words a lot"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import RegexpStemmer\n",
      "#sample for testing the regex stemmer\n",
      "sample_words = \"Helloss Gullivers masters countries yahoos\"\n",
      "my_stemmer = RegexpStemmer('s+$|es$')\n",
      "[my_stemmer.stem(word) for word in sample_words.split()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmed_words[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Helper Functions for Keyphrase related features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#helper function to chunk the sentenses based on a grammar\n",
      "def parse_sents(ip_sent, grammar):\n",
      "    cp = nltk.RegexpParser(grammar)\n",
      "    result = cp.parse(ip_sent)\n",
      "    return result\n",
      "\n",
      "#helper function to extract subtrees\n",
      "def extract_tree(sents,grammar, match):\n",
      "    ret_list = []\n",
      "    for sent in sents:\n",
      "        res = parse_sents(sent, grammar)\n",
      "        ret_list += [' '.join([word for(word, tag) in subtree]) for subtree in res.subtrees(filter = lambda t: t.node == match)]\n",
      "    return ret_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# helper functions for unigrams, bigrams\n",
      "def print_important_nouns(fd, imp_words):\n",
      "    imp_nouns = [((word, count),fd.freq((word,tag))) for ((word, tag), count) in imp_words if re.match(r'N.*', tag)]\n",
      "    top_nouns = imp_nouns[:15]\n",
      "    print '%-16s' % \"NOUNS\", '%-16s' % \"COUNT\", '%-16s' % \"FREQUENCY\\n\"\n",
      "    for ((noun, count), freq) in top_nouns:\n",
      "        print '%-16s' % noun, '%-16s' % count, \"%-16s\" % round((freq * 100), 4)\n",
      "    return imp_nouns\n",
      "\n",
      "def print_important_verbs(fd, imp_words):\n",
      "    imp_verbs = [((word, count),fd.freq((word,tag))) for ((word, tag), count) in imp_words if re.match(r'V+', tag)]\n",
      "    print '%-16s' % \"VERBS\", '%-16s' % \"COUNT\", '%-16s' % \"FREQUENCY\\n\"\n",
      "    top_verbs = imp_verbs[5:20]\n",
      "    for ((verb, count), freq) in top_verbs:\n",
      "        print '%-16s' % verb, '%-16s' % count, \"%-16s\" % round((freq * 100), 4)\n",
      "    return imp_verbs\n",
      "\n",
      "def print_bigrams(stemmed_words):\n",
      "    bigram_fd = nltk.FreqDist(nltk.bigrams([w for w,t in stemmed_words if re.match(r'(JJ|N.*)', t)]))\n",
      "    bigram_fd_items = bigram_fd.items()\n",
      "    bigram_fd_nouns = [(' '.join(map(str,(w1,w2))), c, bigram_fd.freq((w1,w2)) )for ((w1,w2), c) in bigram_fd_items \n",
      "                   if w1 not in string.punctuation and \n",
      "                     w2 not in string.punctuation]\n",
      "\n",
      "    bigram_nouns = [noun for (noun, count) in bigram_fd_items]\n",
      "\n",
      "    print '%-25s' % \"BIGRAM NOUNS\", '%-25s' % \"COUNT\", '%-25s' % \"FREQUENCY\\n\"\n",
      "    for (noun, count, freq) in bigram_fd_nouns[:20]:\n",
      "        print '%-25s' % noun, '%-25s' % count, \"%-25s\" % round((freq * 100), 4)\n",
      "    return bigram_nouns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Colocations\n",
      "def get_bigram_collocations(tag_words):\n",
      "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "    coloc_words = [w for w,t in tag_words if re.match(r'[a-z]+', w)]\n",
      "    finder = BigramCollocationFinder.from_words(coloc_words)\n",
      "    return finder.nbest(bigram_measures.pmi, 20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Naive Bayes Classifier for keyphrase related features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_features(talk):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_feature_sets(feature_function, dataset, split=.9):\n",
      "    featureset = [( feature_function(sent, cat), cat) for (cat, sent)  in dataset ]\n",
      "    return featureset[:int(len(featureset)*split)], featureset[int(len(featureset)*split):]\n",
      "\n",
      "train_set, dev_set = create_feature_sets(create_features, stemmed_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl = nltk.NaiveBayesClassifier.train(train_set)\n",
      "print \"%.3f\" % nltk.classify.accuracy(cl, dev_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Good for testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl.show_most_informative_features(100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_set = create_feature_sets(create_features, stemmed_words, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_kfolds(dataset, k=10):\n",
      "    s = len(dataset)/k\n",
      "    return [ (dataset[:i*s]+dataset[(i+1)*s:], dataset[i*s:(i+1)*s]) for i in range(0, k)]\n",
      "\n",
      "\n",
      "def cross_validate(feature_set, k=10, classifier = nltk.NaiveBayesClassifier):\n",
      "    feature_sets = create_kfolds(feature_set, k)\n",
      "    accuracy = 0.0\n",
      "    index = 0\n",
      "    errors = []\n",
      "    for (train_set, dev_set) in feature_sets:\n",
      "        cl = classifier.train(train_set)\n",
      "        acc = nltk.classify.accuracy(cl, dev_set)\n",
      "        errors += [ (index, i, categories[int(dev_set[i][1])], categories[int(cl.classify(dev_set[i][0]))], dev_set[i][0]) \n",
      "                   for i in range(len(dev_set))\n",
      "                   if cl.classify(dev_set[i][0])!=dev_set[i][1]]\n",
      "        accuracy+=acc\n",
      "        print \"%d\\t%.3f\" % (index, acc)\n",
      "        index +=1\n",
      "    accuracy /= k\n",
      "    print \"Cross Validation Accuracy = %.3f\" % accuracy\n",
      "    return accuracy, errors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Default is Naive Bayes classifier\n",
      "accuracy, errors = cross_validate(feature_set[0], k=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd_w = nltk.FreqDist(words[0])\n",
      "fd_w.items()[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[(',', 234),\n",
        " ('.', 215),\n",
        " ('the', 152),\n",
        " ('to', 81),\n",
        " ('you', 77),\n",
        " ('a', 74),\n",
        " ('of', 73),\n",
        " ('and', 68),\n",
        " ('I', 64),\n",
        " ('``', 61),\n",
        " ('in', 53),\n",
        " ('is', 48),\n",
        " (\"'s\", 47),\n",
        " ('we', 47),\n",
        " ('it', 46),\n",
        " ('that', 46),\n",
        " ('was', 46),\n",
        " (\"n't\", 43),\n",
        " ('And', 42),\n",
        " ('?', 38)]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Discourse Analysis Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tt = nltk.tokenize.texttiling.TextTilingTokenizer()\n",
      "text = ' '.join([' '.join(words) for words in sents[0]])\n",
      "print type(text)\n",
      "# s,ss,d,b = tt.tokenize(text)\n",
      "# misses paragraph breaks..!!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'str'>\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import pylab\n",
      "# pylab.xlabel(\"Sentence Gap index\")\n",
      "# pylab.ylabel(\"Gap Scores\")\n",
      "# pylab.plot(range(len(s)), s, label=\"Gap Scores\")\n",
      "# pylab.plot(range(len(ss)), ss, label=\"Smoothed Gap scores\")\n",
      "# pylab.plot(range(len(d)), d, label=\"Depth scores\")\n",
      "# pylab.stem(range(len(b)),b)\n",
      "# pylab.legend()\n",
      "# pylab.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import wordnet as wn\n",
      "import re\n",
      "w_mphy = [wn.morphy(w,wn.NOUN) for w,t in tagged_words[0] if re.match('N*',t)]\n",
      "fd_wm = nltk.FreqDist(w_mphy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd_wm.items()[:10]\n",
      "# tagged_words[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "[(None, 2648),\n",
        " ('a', 74),\n",
        " ('in', 53),\n",
        " ('it', 47),\n",
        " ('wa', 46),\n",
        " ('think', 28),\n",
        " ('have', 27),\n",
        " ('education', 22),\n",
        " ('are', 20),\n",
        " ('at', 20)]"
       ]
      }
     ],
     "prompt_number": 35
    }
   ],
   "metadata": {}
  }
 ]
}