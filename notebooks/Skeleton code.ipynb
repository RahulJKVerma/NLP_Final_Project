{
 "metadata": {
  "name": "",
  "signature": "sha256:d1bac82324e2658758d02732c115ea6842fe576e6a89947e96a29f7a55f0b154"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Notes\n",
      "1. Get frequent nouns n grams\n",
      "2. Get frequent verb n grams\n",
      "3. Variety in the talk\n",
      "    1. Number of slides - Check for references like (this slide/this video).\n",
      "    2. Laughter \n",
      "    3. More goes here.. \n",
      "\n",
      "## Questions\n",
      "1. Did we lose the paragraph breaks? - Text tiling needs this\n",
      "\n",
      "## Todo\n",
      "1. Would be great to have a method in ted class to extract the metadata."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Classification features\n",
      "1. Punctuations.\n",
      "    1. '?' - thought provoking/call to action\n",
      "    2. '!' - engage audience through expression\n",
      "    3. ';' - could be negative. meaning long sentences\n",
      "2. Collocations:\n",
      "    1. Bigrams or collocated nouns/verbs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import pickle\n",
      "import nltk\n",
      "import string\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import json, urllib, csv\n",
      "from nltk.corpus import wordnet as wn\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn import datasets, linear_model\n",
      "sys.path.append('../Scripts')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# loading data from pickle files (tokenization & tagging- default nltk)\n",
      "import ted_object\n",
      "from ted_object import ted\n",
      "t_obj = ted()\n",
      "\n",
      "tagged_sents = t_obj.tagged_sents_talk()\n",
      "tagged_words = t_obj.tagged_words_talk()\n",
      "tagged_paras = t_obj.tagged_paras()\n",
      "#sents = t_obj.sents_talk() Rahul: For some reason these statments are not working and hence commenting them for the time being.\n",
      "#words = t_obj.words_talk()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Category 1: Confusing, 2: Funny, 3: Informative\n",
      "# We said we would have 3 binary classifiers\n",
      "def read_category_labels():\n",
      "    rating_file = open('../Corpus/User Reviews.csv', 'rb')\n",
      "    count = 0\n",
      "    categ_list = list()\n",
      "    for line in rating_file:\n",
      "        item = line.split(',')\n",
      "        count += 1\n",
      "#         categ = int(item[15])*1 + int(item[16])*2 + int(item[17]) * 3\n",
      "        categ_list.append([item[15], item[16], item[17]])\n",
      "    return categ_list\n",
      "    #     print count, [item[15], item[16], item[17]], categ\n",
      "    # categ_list\n",
      "    \n",
      "# Here's category labels for regression (percentages instead of [0 or 1])\n",
      "all_labels = pickle.load(open('../Corpus/y.p', 'rb'))\n",
      "confusing_y = [row[0] for row in all_labels]\n",
      "funny_y = [row[1] for row in all_labels]\n",
      "informative_y = [row[2] for row in all_labels]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(confusing_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "177"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Clean up text: Stop word removal & Stemming - Code here if we need to revisit the current stemming"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import *\n",
      "from nltk.stem.porter import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenize_text(text):\n",
      "    pattern =[\"(?x)([A-Z]\\.)+\",\n",
      "               \"\\w+([-]\\w+)*\",\n",
      "               \"\\$?\\d+(\\.\\d+)?%?\",\n",
      "               \"\\.\\.\\.\",\n",
      "               \"[.,?;]+\"]\n",
      "    pattern = \"|\".join(pattern)\n",
      "    text = \" \".join(text) if isinstance(text, list) == True else text\n",
      "    tokens = nltk.regexp_tokenize(text,pattern)\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wnl = WordNetLemmatizer()\n",
      "\n",
      "# reduce words to root format\n",
      "def get_root_phrase(word, tag):\n",
      "    word = word.lower()\n",
      "    #trim joint words\n",
      "    if word.endswith(\"'s\"):\n",
      "        print word\n",
      "        word = word[:-2]\n",
      "    #don't cut short words. e.g. rss would be reduced to r\n",
      "    if len(word)<=3 and word not in COMMON_WORDS:\n",
      "        return word\n",
      "    #try to morphy\n",
      "    morphy = wn.morphy(word)\n",
      "    if morphy is None:\n",
      "        morphy = word\n",
      "    #lemmatize morphy\n",
      "    lem = wnl.lemmatize(morphy, 'n')\n",
      "    if lem and len(word)>3: \n",
      "        morphy = lem\n",
      "    return morphy\n",
      "\n",
      "# Get morphy words in a sentence\n",
      "def get_morphys(tagged_sentence):\n",
      "   return [ (get_root_phrase(word, tag), tag) for (word, tag) in tagged_sentence \\\n",
      "               if word not in stop_words and (word.isalnum() or '-' in word) and not word.isdigit()\\\n",
      "               and len(get_root_phrase(word, tag))>1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#remove all stop words and punctuations\n",
      "non_stop_words = list()\n",
      "for talk in tagged_words:\n",
      "    non_stop_words.append([(w,t) for w,t in talk if w.lower() not in stopwords.words('english') and\n",
      "                       w.lower() not in string.punctuation])\n",
      "pickle.dump(non_stop_words, open( \"../Corpus/non_stop_tagged_words_talks.p\", \"wb\" ))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "porter_stemmer = PorterStemmer()\n",
      "stemmed_words = list()\n",
      "for talk in non_stop_words:\n",
      "    stemmed_words.append([(porter_stemmer.stem(w),t) for w,t in talk])\n",
      "pickle.dump(stemmed_words, open( \"../Corpus/stemmed_tagged_words_talks.p\", \"wb\" ))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Helper Functions for Chunking"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#helper function to chunk the sentenses based on a grammar\n",
      "def parse_sents(ip_sent, grammar):\n",
      "    cp = nltk.RegexpParser(grammar)\n",
      "    result = cp.parse(ip_sent)\n",
      "    return result\n",
      "\n",
      "#helper function to extract subtrees\n",
      "def extract_tree(sents,grammar, match):\n",
      "    ret_list = []\n",
      "    for sent in sents:\n",
      "        res = parse_sents(sent, grammar)\n",
      "        ret_list += [' '.join([word for(word, tag) in subtree]) for subtree in res.subtrees(filter = lambda t: t.node == match)]\n",
      "    return ret_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Naive Bayes Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "category_list = read_category_labels()\n",
      "confusing_set = list()\n",
      "funny_set = list()\n",
      "info_set = list()\n",
      "for i in range(len(category_list)):\n",
      "    confusing_set.append((category_list[i][0], tagged_sents[i]))\n",
      "    funny_set.append((category_list[i][1], tagged_sents[i]))\n",
      "    info_set.append((category_list[i][2], tagged_sents[i]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(category_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "177"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: All number features should be bucketed to a range. ##\n",
      "BUCKET_RANGE = 10    # how about min-max normalization (range 0-1) ?\n",
      "\n",
      "def min_max_normalize(feature):\n",
      "    '''Converts a list of numberical values to a list of values between 0 and 1'''\n",
      "    minVal = min(feature)\n",
      "    maxVal = max(feature)\n",
      "    normalized = [(e - minVal / (maxVal - minVal)) for e in feature]\n",
      "    return normalized\n",
      "    \n",
      "def avg_sent_len(talk_sents, flag = 'classifier'):\n",
      "    \"\"\"The flag is used to specify the format of the return value\n",
      "    Classifier would use a different format as compared to regression. The default\n",
      "    value is classifier\"\"\"\n",
      "    num_sents = len(talk_sents)\n",
      "    total_len = sum([len(sent) for sent in talk_sents])\n",
      "    avg_len = (total_len)/(num_sents * BUCKET_RANGE)\n",
      "    if flag == 'classifier':\n",
      "        return [('AVGLEN_%s' %(avg_len), 1)]\n",
      "    else:\n",
      "        return[('AVGLEN', avg_len)]\n",
      "\n",
      "def punctuation_count(talk_sents, punc, flag = 'classifier'):\n",
      "    \"\"\"The flag is used to specify the format of the return value\n",
      "    Classifier would use a different format as compared to regression. The default\n",
      "    value is classifier\"\"\"\n",
      "    num_punc = sum([1 for sent in talk_sents for w,t in sent if punc == w])\n",
      "    if flag == 'classifier':\n",
      "        return [('%s_%s' %(punc,num_punc/BUCKET_RANGE), 1)]\n",
      "    else:\n",
      "        return [(punc,num_punc)]\n",
      "\n",
      "# From the paper Rahul mentioned for Technical terms - Citation required\n",
      "def num_techterms(talk_sents, flag = 'classifier'):\n",
      "    \"\"\"The flag is used to specify the format of the return value\n",
      "    Classifier would use a different format as compared to regression. The default\n",
      "    value is classifier\"\"\"\n",
      "    njn_list = extract_tree(talk_sents, 'NJN: {(<N.*>*<J*>*<N.*>+)+}','NJN')\n",
      "    npn_list = extract_tree(talk_sents, 'NPN: {(<N.*>+<P.*><N.*>+)+}','NPN')\n",
      "    if flag == 'classifier':\n",
      "        return [('NUMTECH_%s' %((len(njn_list) + len(npn_list))/BUCKET_RANGE), 1)]\n",
      "    else:\n",
      "        return [('NUMTECH', len(njn_list) + len(npn_list))]\n",
      "\n",
      "def num_first_person_pronoun_chunk(talk_sents, flag = 'classifier'):\n",
      "    \"\"\"The flag is used to specify the format of the return value\n",
      "    Classifier would use a different format as compared to regression. The default\n",
      "    value is classifier\"\"\"\n",
      "    pro_verb = \"(<PR.*>|<N.*P>+)+<TO>?<MD>?<TO>?<VB.*>+\"\n",
      "    VB_grammar = \"PROVB: {\" + pro_verb + \"}\"\n",
      "    pvb_list = extract_tree(talk_sents,VB_grammar, \"PROVB\")\n",
      "    if flag == 'classifier':\n",
      "        return [('NUMFPRO_%s' %(len(pvb_list)/BUCKET_RANGE), 1)]\n",
      "    else:\n",
      "        return [('NUMFPRO', len(pvb_list))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Possible Features:\n",
      "- **Confusing** and **Informative**\n",
      "    - total number of words\n",
      "    - average word length\n",
      "    - vocabulary richness (total unique words / N)\n",
      "    - number of words longer than 10 characters\n",
      "    - number of sentences\n",
      "    - number of paragraphs\n",
      "    - average words per sentence\n",
      "    - average words per paragraph\n",
      "    - number of adjectives\n",
      "    - number of uncertainty verbs (wonder, consider, suppose)\n",
      "    - negations (no, not, never)\n",
      "    - certainty (always, never)\n",
      "    - hedges (well, kind of, sort of, possibly, maybe)\n",
      "    - number of '?'\n",
      "    - number of '!'\n",
      "    - number of technical terms\n",
      "    - average pronouns per paragraph\n",
      "    - number of conjunctions\n",
      "    - number of prepositions\n",
      "\n",
      "- **Funny**\n",
      "    - number of interjections (wow, yeah)\n",
      "    - intensive adverbs (really, very, quite, special)\n",
      "    - positive emotion (love, nice, sweet)\n",
      "    - negative emotion (hurt, ugly, nasty)\n",
      "    - number of annotated laughs in transcript\n",
      "    - number of adjectives\n",
      "    - number of verbs\n",
      "    - number of '?'\n",
      "    - number of '!'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def con_inf_features(talk_sents, flag = 'classifier'):\n",
      "    \"\"\"The flag is used to specify the format of the return value\n",
      "    Classifier would use a different format as compared to regression. The default\n",
      "    value of flag is classifier\"\"\"\n",
      "    feature_list = list()\n",
      "    feature_list.extend(avg_sent_len(talk_sents, flag))\n",
      "    feature_list.extend(punctuation_count(talk_sents, '!', flag))\n",
      "    feature_list.extend(punctuation_count(talk_sents, '?', flag))\n",
      "    feature_list.extend(punctuation_count(talk_sents, ';', flag))\n",
      "    feature_list.extend(num_techterms(talk_sents, flag))\n",
      "    #feature_list.extend(num_first_person_pronoun_chunk(talk_sents, flag)) Taking a long time to execute hence commenting out temporarily\n",
      "    feature_set = dict(feature_list)\n",
      "    return feature_set\n",
      "\n",
      "def funny_features(talk_sents):\n",
      "    \"\"\"The flag is used to specify the format of the return value\n",
      "    Classifier would use a different format as compared to regression. The default\n",
      "    value is classifier\"\"\"\n",
      "    feature_list = list()\n",
      "    feature_list.extend()\n",
      "    return "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Category information should be encoded. For now everything is 1\n",
      "def create_feature_sets(feature_function, type_variable, flag='classifier', split=[.7,.2,.1]):\n",
      "    \"\"\" The type variable is used to define whether we should use the info set, confusing set or the funny set. We have used a variable\n",
      "    so that we can also include the appropriate percentages as the label for regression. We have not merged the percentages together \n",
      "    to the main x variables so that one split can be used to segregate training, validation and testing dataframes instead\n",
      "    of having to split them separately\"\"\"\n",
      "    dataset = confusing_set if type_variable == 'confusing_set' else info_set if type_variable == 'info_set' else funny_set\n",
      "    featureset = [ (feature_function(talk_sents, flag), categ) for (categ, talk_sents) in dataset ]\n",
      "    if flag != 'classifier': # If the featureset needed is in a dataframe format\n",
      "        temp_df = pd.DataFrame.from_dict([value[0] for value in featureset])\n",
      "        temp_df['Label'] = confusing_set if type_variable == 'confusing_set' else funny_set if type_variable == 'funny_set' else informative_set\n",
      "        featureset = temp_df\n",
      "    trainlen = int(split[0] * len(featureset))\n",
      "    devlen = int(split[1] * len(featureset))\n",
      "    return featureset[:trainlen], featureset[trainlen:trainlen+devlen], featureset[trainlen:trainlen+devlen:]\n",
      "#     return featureset[:int(len(featureset)*split)], featureset[int(len(featureset)*split):]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Please set the flag as per the type of model you are using. Classifier can use 'classifier' as flag and regression needs to \n",
      "# specify the flag as 'regression' so that appropriate feature set can be generated.\n",
      "train_set_con, dev_set_con, test_set_con = create_feature_sets(con_inf_features, 'confusing_set', 'classifier') # Please set this flag!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl_con = nltk.NaiveBayesClassifier.train(train_set_con)\n",
      "print \"%.3f\" % nltk.classify.accuracy(cl_con, dev_set_con)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set_fun, dev_set_fun, test_set_fun = create_feature_sets(create_features, 'funny_set', 'classifier') # Please set this flag!\n",
      "cl_fun = nltk.NaiveBayesClassifier.train(train_set_fun)\n",
      "print \"%.3f\" % nltk.classify.accuracy(cl_fun, dev_set_fun)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.524\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set_info, dev_set_info, test_set_info = create_feature_sets(create_features, 'info_set', 'classifier') # Please set this flag\n",
      "cl_info = nltk.NaiveBayesClassifier.train(train_set_info)\n",
      "print \"%.3f\" % nltk.classify.accuracy(cl_info, dev_set_info)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.333\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Good for testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl_con.show_most_informative_features(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl_fun.show_most_informative_features(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl_info.show_most_informative_features(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Regression"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Converting feature sets into dataframes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set_con, dev_set_con, test_set_con = create_feature_sets(con_inf_features, 'confusing_set', 'regression') # Please set this flag!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(train_set_con.iloc[:,:-1], train_set_con.iloc[:,-1])\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "# The mean square error\n",
      "print(\"Residual sum of squares: %.2f\"\n",
      "      % np.mean((regr.predict(dev_set_con.iloc[:,:-1]) - dev_set_con.iloc[:,-1]) ** 2))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % regr.score(dev_set_con.iloc[:,:-1], dev_set_con.iloc[:,-1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set_con"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_set = create_feature_sets(create_features, confusing_set, [1,0,0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_kfolds(dataset, k=10):\n",
      "    s = len(dataset)/k\n",
      "    return [ (dataset[:i*s]+dataset[(i+1)*s:], dataset[i*s:(i+1)*s]) for i in range(0, k)]\n",
      "\n",
      "\n",
      "def cross_validate(feature_set, k=10, classifier = nltk.NaiveBayesClassifier):\n",
      "    feature_sets = create_kfolds(feature_set, k)\n",
      "    accuracy = 0.0\n",
      "    index = 0\n",
      "    errors = []\n",
      "    for (train_set, dev_set) in feature_sets:\n",
      "        cl = classifier.train(train_set)\n",
      "        acc = nltk.classify.accuracy(cl, dev_set)\n",
      "        errors += [ (i, int(dev_set[i][1]), int(cl.classify(dev_set[i][0])), dev_set[i][0]) for i in range(len(dev_set))\n",
      "                   if cl.classify(dev_set[i][0])!=dev_set[i][1]]\n",
      "        accuracy+=acc\n",
      "        print \"%d\\t%.3f\" % (index, acc)\n",
      "        index +=1\n",
      "    accuracy /= k\n",
      "    print \"Cross Validation Accuracy = %.3f\" % accuracy\n",
      "    return accuracy, errors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Default is Naive Bayes classifier\n",
      "accuracy, errors = cross_validate(feature_set[0], k=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\t0.200\n",
        "1\t0.600\n",
        "2\t1.000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3\t0.400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4\t0.600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5\t0.400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6\t0.600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7\t0.600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8\t0.600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9\t0.400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10\t0.600\n",
        "11\t0.600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "12\t0.600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "13\t0.600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "14\t0.200\n",
        "15\t0.600\n",
        "16\t0.400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17\t0.600\n",
        "18\t0.400\n",
        "19\t0.400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross Validation Accuracy = 0.520\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Discourse Analysis Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tt = nltk.tokenize.texttiling.TextTilingTokenizer()\n",
      "text = ' '.join([' '.join(words) for words in sents[0]])\n",
      "print type(text)\n",
      "# s,ss,d,b = tt.tokenize(text)\n",
      "# misses paragraph breaks..!!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import pylab\n",
      "# pylab.xlabel(\"Sentence Gap index\")\n",
      "# pylab.ylabel(\"Gap Scores\")\n",
      "# pylab.plot(range(len(s)), s, label=\"Gap Scores\")\n",
      "# pylab.plot(range(len(ss)), ss, label=\"Smoothed Gap scores\")\n",
      "# pylab.plot(range(len(d)), d, label=\"Depth scores\")\n",
      "# pylab.stem(range(len(b)),b)\n",
      "# pylab.legend()\n",
      "# pylab.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Code just put in - delete later if not required"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Might be useful if the Porter Stemmer distorts the words a lot"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import RegexpStemmer\n",
      "#sample for testing the regex stemmer\n",
      "sample_words = \"Helloss Gullivers masters countries yahoos\"\n",
      "my_stemmer = RegexpStemmer('s+$|es$')\n",
      "[my_stemmer.stem(word) for word in sample_words.split()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# helper functions for unigrams, bigrams\n",
      "def print_important_nouns(fd, imp_words):\n",
      "    imp_nouns = [((word, count),fd.freq((word,tag))) for ((word, tag), count) in imp_words if re.match(r'N.*', tag)]\n",
      "    top_nouns = imp_nouns[:15]\n",
      "    print '%-16s' % \"NOUNS\", '%-16s' % \"COUNT\", '%-16s' % \"FREQUENCY\\n\"\n",
      "    for ((noun, count), freq) in top_nouns:\n",
      "        print '%-16s' % noun, '%-16s' % count, \"%-16s\" % round((freq * 100), 4)\n",
      "    return imp_nouns\n",
      "\n",
      "def print_important_verbs(fd, imp_words):\n",
      "    imp_verbs = [((word, count),fd.freq((word,tag))) for ((word, tag), count) in imp_words if re.match(r'V+', tag)]\n",
      "    print '%-16s' % \"VERBS\", '%-16s' % \"COUNT\", '%-16s' % \"FREQUENCY\\n\"\n",
      "    top_verbs = imp_verbs[5:20]\n",
      "    for ((verb, count), freq) in top_verbs:\n",
      "        print '%-16s' % verb, '%-16s' % count, \"%-16s\" % round((freq * 100), 4)\n",
      "    return imp_verbs\n",
      "\n",
      "def print_bigrams(stemmed_words):\n",
      "    bigram_fd = nltk.FreqDist(nltk.bigrams([w for w,t in stemmed_words if re.match(r'(JJ|N.*)', t)]))\n",
      "    bigram_fd_items = bigram_fd.items()\n",
      "    bigram_fd_nouns = [(' '.join(map(str,(w1,w2))), c, bigram_fd.freq((w1,w2)) )for ((w1,w2), c) in bigram_fd_items \n",
      "                   if w1 not in string.punctuation and \n",
      "                     w2 not in string.punctuation]\n",
      "\n",
      "    bigram_nouns = [noun for (noun, count) in bigram_fd_items]\n",
      "\n",
      "    print '%-25s' % \"BIGRAM NOUNS\", '%-25s' % \"COUNT\", '%-25s' % \"FREQUENCY\\n\"\n",
      "    for (noun, count, freq) in bigram_fd_nouns[:20]:\n",
      "        print '%-25s' % noun, '%-25s' % count, \"%-25s\" % round((freq * 100), 4)\n",
      "    return bigram_nouns\n",
      "\n",
      "def print_ngram_nouns(stemmed_words):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd_w = nltk.FreqDist(tagged_words[0])\n",
      "imp_nouns = print_important_nouns(fd_w, fd_w.items()[:200])\n",
      "imp_verbs = print_important_verbs(fd_w, fd_w.items()[:200])\n",
      "imp_bigrams = print_bigrams(tagged_words[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Colocations\n",
      "def get_bigram_collocations(tag_words):\n",
      "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "    coloc_words = [w for w,t in tag_words if re.match(r'[a-z]+', w)]\n",
      "    finder = BigramCollocationFinder.from_words(coloc_words)\n",
      "    return finder.nbest(bigram_measures.pmi, 20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Rahul: Adding code for collocation using the POS. Can be deleted if another version already exists above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def PatternBasedCollocationAAN(sentence):  #Finding terms like \"Gaussian random variable\"\n",
      "    grammar = r\"\"\"\n",
      "      AAN: {<JJ.*><JJ.*><NN.*|NP.*>}   # Identify terms\n",
      "    \"\"\"\n",
      "    cp = nltk.RegexpParser(grammar)\n",
      "    result = cp.parse(sentence)\n",
      "    return result\n",
      "\n",
      "def PatternBasedCollocationNNN(sentence):  #Finding terms like \"class probability function\"\n",
      "    grammar = r\"\"\"\n",
      "      NNN: {<NN.*|NP.*><NN.*|NP.*><NN.*|NP.*>}   # Identify terms\n",
      "    \"\"\"\n",
      "    cp = nltk.RegexpParser(grammar)\n",
      "    result = cp.parse(sentence)\n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "AAN_collocations = []\n",
      "collocations =  PatternBasedCollocationAAN(tagged_text[0])\n",
      "for collocation in collocations.subtrees():\n",
      "        if collocation.label() == 'AAN':\n",
      "            lst = []\n",
      "            for leaf in collocation.leaves():\n",
      "                lst.append(leaf[0])\n",
      "            AAN_collocations.append(' '.join(lst))\n",
      "            \n",
      "NNN_collocations_text = []\n",
      "collocations =  PatternBasedCollocationNNN(tagged_text[0])\n",
      "for collocation in collocations.subtrees():\n",
      "        if collocation.label() == 'NNN':\n",
      "            lst = []\n",
      "            for leaf in collocation.leaves():\n",
      "                lst.append(leaf[0])\n",
      "            NNN_collocations_text.append(' '.join(lst))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}